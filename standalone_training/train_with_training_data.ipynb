{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48baaf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "import xgboost as xgb\n",
    "#from statsmodels.tsa.arima.model import ARIMA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "def calculate_rsi(data, window=14):\n",
    "    delta = data['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss.replace(0, 1e-6)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def feature_engineering(df):\n",
    "    import numpy as np\n",
    "    \"\"\"\n",
    "    Takes a preprocessed DataFrame and creates all necessary time-series features.\n",
    "    \"\"\"\n",
    "    print(\"--- Performing Feature Engineering ---\")\n",
    "    if len(df) < 15:\n",
    "        error_msg = f'Dataframe size is less than 15 ({len(df)} rows found). Cannot create features. Get more data.'\n",
    "        print(f\"ðŸš¨ {error_msg}\")\n",
    "        raise ValueError(error_msg) # Stop execution here\n",
    "        \n",
    "    df_feat = df.copy()\n",
    "    df_feat['rsi_14'] = calculate_rsi(df_feat, window=14)\n",
    "    df_feat['lag_1'] = df_feat['close'].shift(1)\n",
    "    df_feat['lag_5'] = df_feat['close'].shift(5)\n",
    "    df_feat['lag_10'] = df_feat['close'].shift(10)\n",
    "    df_feat['rolling_mean_5'] = df_feat['close'].rolling(window=5).mean()\n",
    "    df_feat['rolling_std_5'] = df_feat['close'].rolling(window=5).std()\n",
    "    df_feat['rolling_mean_10'] = df_feat['close'].rolling(window=10).mean()\n",
    "    df_feat['rolling_std_10'] = df_feat['close'].rolling(window=10).std()\n",
    "    log_return = np.log(df_feat['close'] / df_feat['close'].shift(1))\n",
    "    df_feat['volatility_7'] = log_return.rolling(window=7).std() # 7-day volatility\n",
    "    df_feat['momentum_5'] = log_return.rolling(window=5).mean() # 5-day momentum (average return)\n",
    "    df_feat['rsi_14'] = calculate_rsi(df_feat, window=14)\n",
    "    df_feat['day_of_week'] = df_feat['merge_date'].dt.dayofweek\n",
    "    df_feat['high_low_spread'] = df_feat['high'] - df_feat['low']\n",
    "    df_feat['momentum_x_volume'] = df_feat['momentum_5'] * df_feat['volume']\n",
    "    df_feat['rsi_sq'] = df_feat['rsi_14']**2\n",
    "    \n",
    "    return df_feat\n",
    "\n",
    "df = pd.read_csv(r'/kaggle/input/bitcoin-and-us-treasury-with-daily-sentiment/bitcoin_sentiment_12012022_11082025.csv')\n",
    "df['target'] = df['close'].shift(-1)\n",
    "df['merge_date'] = pd.to_datetime(df['merge_date'], errors='coerce')\n",
    "df = feature_engineering(df)\n",
    "\n",
    "# --- 1. Define Features ---\n",
    "# The 'close' column is now a feature, not the target.\n",
    "# We must exclude the new 'target' column from the features.\n",
    "features = [col for col in df.columns if col not in ['merge_date', 'datetime_utc', 'timestamp', 'target'] and 'unnamed' not in col]\n",
    "\n",
    "# --- 2. Clean Data ---\n",
    "# Define the columns that MUST NOT have NaNs for training\n",
    "required_cols = [col for col in df.columns if col not in ['merge_date', 'datetime_utc', 'timestamp', 'unnamed'] and 'unnamed' not in col]\n",
    "required_cols.append('target') # Ensure the new target is included\n",
    "\n",
    "# Drop rows based on the required columns\n",
    "data_clean = df.dropna(subset=required_cols).reset_index(drop=True)\n",
    "\n",
    "# Check the result after cleaning\n",
    "if data_clean.empty:\n",
    "    raise ValueError(\"The dataset is empty after dropping NaNs. Check your feature engineering steps for excessive NaN creation!\")\n",
    "\n",
    "X = data_clean[features]\n",
    "y = data_clean['target'] # Use the new 'target' column\n",
    "\n",
    "# --- 3. Split and Train ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "\n",
    "selected_features = ['open', 'high', 'low', 'close', 'volume', 'weighted_sentiment', 'lag_1', 'lag_10', \n",
    "                     'rolling_mean_5', 'rolling_mean_10', 'rolling_std_10', 'volatility_7', 'momentum_5', 'high_low_spread', 'momentum_x_volume', 'rsi_sq']\n",
    "X_train = X_train[selected_features]   \n",
    "X_test = X_test[selected_features]\n",
    "features = selected_features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "########################\n",
    "\n",
    "print(\"--- Training and Evaluating Models ---\")\n",
    "results = {}\n",
    "\n",
    "\n",
    "# 4. Train ML Models\n",
    "mlr_model = LinearRegression().fit(X_train_scaled, y_train)\n",
    "ridge_model = Ridge(alpha=0.5, random_state=42).fit(X_train_scaled, y_train)\n",
    "rf_model = RandomForestRegressor(n_estimators=600, random_state=42).fit(X_train_scaled, y_train)\n",
    "xgb_model = xgb.XGBRegressor(colsample_bytree=1.0, learning_rate=0.03, max_depth= 7,n_estimators=1200, subsample = 0.9,\n",
    "                             objective='reg:squarederror', random_state=42).fit(X_train_scaled, y_train)\n",
    "\n",
    "# Train Auto-ARIMA on the training data for a fair comparison\n",
    "#arima_model = ARIMA(y_train, order=(1,1,3))\n",
    "#arima_model_fit = arima_model.fit()\n",
    "\n",
    "# Evaluate Models\n",
    "mlr_preds = mlr_model.predict(X_test_scaled)\n",
    "ridge_preds = ridge_model.predict(X_test_scaled)\n",
    "rf_preds = rf_model.predict(X_test_scaled)\n",
    "xgb_preds = xgb_model.predict(X_test_scaled)\n",
    "#arima_preds = arima_model_fit.forecast(steps=len(y_test))\n",
    "\n",
    "results['Linear Regression'] = {'MAE': mean_absolute_error(y_test, mlr_preds)}\n",
    "results['Ridge Regression (0.5)'] = {'MAE': mean_absolute_error(y_test, ridge_preds)}\n",
    "results['Random Forest'] = {'MAE': mean_absolute_error(y_test, rf_preds)}\n",
    "results['XGBoost'] = {'MAE': mean_absolute_error(y_test, xgb_preds)}\n",
    "\n",
    "#dropped ARIMA model. Models with more context are outperforming ARIMA. \n",
    "#results[f'ARIMA (1,1,3)'] = {'MAE': mean_absolute_error(y_test, arima_preds)}\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\n--- Model Performance on Test Set ---\")\n",
    "print(results_df.sort_values(by='MAE'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
