{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32c6aad",
   "metadata": {},
   "source": [
    "# Transformer Neural Network - DO NOT USE. NOT ENOUGH DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c6012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split # ### <<< CHANGE: Not used, will do time-based split\n",
    "from itertools import combinations\n",
    "import copy\n",
    "\n",
    "df \n",
    "# # --- Load Data ---\n",
    "# try:\n",
    "#     df = pd.read_csv(r'/dbfs/mnt/four18_s3/Other/1761568170.364943_year_hourly.csv')\n",
    "# except FileNotFoundError:\n",
    "#     print(\"File not found. Using dummy data for demonstration.\")\n",
    "#     # Create dummy data if file not found\n",
    "#     dates = pd.date_range(start='2023-01-01', periods=8760, freq='H') # 1 year\n",
    "#     data = {\n",
    "#         'datetime_utc': dates,\n",
    "#         'open': np.random.uniform(30000, 50000, 8760),\n",
    "#         'high': lambda x: x['open'] + np.random.uniform(0, 1000, 8760),\n",
    "#         'low': lambda x: x['open'] - np.random.uniform(0, 1000, 8760),\n",
    "#         'close': lambda x: x['open'] + np.random.uniform(-500, 500, 8760),\n",
    "#         'volume': np.random.uniform(10, 1000, 8760)\n",
    "#     }\n",
    "#     df = pd.DataFrame(data)\n",
    "#     df['high'] = df.apply(lambda row: row['open'] + np.random.uniform(0, 1000), axis=1)\n",
    "#     df['low'] = df.apply(lambda row: row['open'] - np.random.uniform(0, 1000), axis=1)\n",
    "#     df['close'] = df.apply(lambda row: row['open'] + np.random.uniform(-500, 500), axis=1)\n",
    "\n",
    "\n",
    "# Hold out the *actual* next hour you want to predict\n",
    "true_close = df['close'].iloc[-1]\n",
    "# The rest of the data is for training and validation\n",
    "historical_df = df.iloc[:-1].copy()\n",
    "\n",
    "# --- Feature engineering ---\n",
    "def create_transformer_features(df, sentiment_df=None):\n",
    "    # Ensure 'datetime_utc' exists, even for dummy data\n",
    "    if 'datetime_utc' not in df.columns:\n",
    "        df['datetime_utc'] = pd.date_range(start='2023-01-01', periods=len(df), freq='H')\n",
    "\n",
    "    df = df.copy().sort_values('datetime_utc').reset_index(drop=True)\n",
    "    df['ma7'] = df['close'].rolling(7).mean()\n",
    "    df['ma3'] = df['close'].rolling(3).mean()\n",
    "    \n",
    "    # Consider adding time-based features\n",
    "    df['hour'] = pd.to_datetime(df['datetime_utc']).dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['datetime_utc']).dt.dayofweek\n",
    "\n",
    "    # Keep numeric OHLCV\n",
    "    df_feat = df[['open','high','low','close','volume','hour','day_of_week']].copy()\n",
    "    \n",
    "\n",
    "    # # 2nd order interactions (optional)\n",
    "    # for f1, f2 in combinations(df_feat.columns, 2):\n",
    "    #     df_feat[f\"{f1}_x_{f2}\"] = df_feat[f1] * df_feat[f2]\n",
    "    #     df_feat[f\"{f1}_div_{f2}\"] = np.where(df_feat[f2]!=0, df_feat[f1]/df_feat[f2], 0)\n",
    "\n",
    "    # Other technical features\n",
    "    df_feat['log_return'] = np.log(df_feat['close']/df_feat['close'].shift(1))\n",
    "    df_feat['range'] = df_feat['high'] - df_feat['low']\n",
    "\n",
    "    # Drop the first row that has NaN from log_return\n",
    "    df_feat = df_feat.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Optional sentiment merge\n",
    "    if sentiment_df is not None:\n",
    "        sentiment_df = sentiment_df.copy()\n",
    "        sentiment_df['date'] = pd.to_datetime(sentiment_df['datetime_utc']).dt.date\n",
    "        # Need to align dates. Assuming df_feat needs its date column from the original df\n",
    "        # This part is tricky without seeing the original df index alignment\n",
    "        original_dates = pd.to_datetime(df['datetime_utc']).iloc[df_feat.index]\n",
    "        df_feat['date'] = original_dates.dt.date\n",
    "        df_feat = df_feat.merge(sentiment_df[['date','weighted_sentiment']], on='date', how='left')\n",
    "        df_feat = df_feat.drop(columns=['date'])\n",
    "        # Handle potential NaNs from merge\n",
    "        df_feat['weighted_sentiment'] = df_feat['weighted_sentiment'].fillna(0)\n",
    "\n",
    "\n",
    "    feature_columns = df_feat.columns.tolist()\n",
    "    return df_feat, feature_columns\n",
    "\n",
    "# --- Dataset ---\n",
    "### <<< CHANGE: Refactored Dataset to accept pre-fit scalers\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df_features, df_target, window_size, feature_scaler, target_scaler):\n",
    "        self.window_size = window_size\n",
    "        self.feature_scaler = feature_scaler\n",
    "        self.target_scaler = target_scaler\n",
    "\n",
    "        # Transform the data \n",
    "        self.data_scaled = self.feature_scaler.transform(df_features)\n",
    "        self.target_scaled = self.target_scaler.transform(df_target)\n",
    "\n",
    "    def __len__(self):\n",
    "        # We need window_size history + 1 target\n",
    "        return len(self.data_scaled) - self.window_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Features\n",
    "        x = self.data_scaled[idx:idx+self.window_size]\n",
    "\n",
    "        # Target\n",
    "        # We want the 'close' price at the end of the window\n",
    "        y = self.target_scaled[idx+self.window_size]\n",
    "\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# --- Transformer model ---\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    ### <<< CHANGE: Added window_size to init to make positional encoding dynamic\n",
    "    def __init__(self, num_features, window_size, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_fc = nn.Linear(num_features, d_model)\n",
    "\n",
    "        # Positional Encoding (highly recommended for Transformers)\n",
    "        # Using a simple learned embedding here, but sinusoidal is also common\n",
    "        ### <<< CHANGE: num_embeddings is now window_size\n",
    "        self.pos_encoder = nn.Embedding(window_size, d_model) # Assuming max seq_len < 100\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_fc = nn.Linear(d_model, 1)\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, features]\n",
    "\n",
    "        # --- Add Positional Encoding ---\n",
    "        # Create positions: [0, 1, 2, ..., seq_len-1]\n",
    "        seq_len = x.size(1)\n",
    "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0) # [1, seq_len]\n",
    "        pos_emb = self.pos_encoder(positions) # [1, seq_len, d_model]\n",
    "\n",
    "        x = self.input_fc(x)                 # -> [batch, seq_len, d_model]\n",
    "        x = x + pos_emb                      # Add positional encoding\n",
    "        x = self.norm(x)  # Add normalization\n",
    "\n",
    "        x = self.transformer_encoder(x)      # -> [batch, seq_len, d_model]\n",
    "\n",
    "        # Get the representation of the *last* time step\n",
    "        x = x[:, -1, :]                      # -> [batch, d_model]\n",
    "        out = self.output_fc(x)              # -> [batch, 1]\n",
    "        return out\n",
    "\n",
    "# --- Training function ---\n",
    "### <<< CHANGE: Added validation loop and best model saving\n",
    "def train_transformer(train_df, val_df, feature_cols, target_col, window_size,\n",
    "                      epochs=50, batch_size=64, lr=1e-3, device='cuda',d_model=64, num_layers=2):\n",
    "\n",
    "    # --- 1. Fit Scalers ONLY on training data ---\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit scalers\n",
    "    feature_scaler.fit(train_df[feature_cols])\n",
    "    target_scaler.fit(train_df[[target_col]])\n",
    "\n",
    "    # --- 2. Create Datasets and DataLoaders ---\n",
    "    train_dataset = TimeSeriesDataset(\n",
    "        df_features=train_df[feature_cols],\n",
    "        df_target=train_df[[target_col]],\n",
    "        window_size=window_size,\n",
    "        feature_scaler=feature_scaler,\n",
    "        target_scaler=target_scaler\n",
    "    )\n",
    "\n",
    "    val_dataset = TimeSeriesDataset(\n",
    "        df_features=val_df[feature_cols],\n",
    "        df_target=val_df[[target_col]],\n",
    "        window_size=window_size,\n",
    "        feature_scaler=feature_scaler,\n",
    "        target_scaler=target_scaler\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # --- 3. Initialize Model, Loss, Optimizer ---\n",
    "    ### <<< CHANGE: Pass window_size to the model\n",
    "    model = TimeSeriesTransformer(num_features=len(feature_cols), window_size=window_size, d_model=d_model,num_layers=num_layers).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.SmoothL1Loss() #nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "    # --- 4. Training & Validation Loop ---\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # --- Validation Loop ---\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                y_pred = model(x_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"  -> New best model saved with Val Loss: {best_val_loss:.6f}\")\n",
    "            counter = 0\n",
    "        else: counter+=1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Load the best model state\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Return the *best* model and the *fitted* scalers\n",
    "    return model, feature_scaler, target_scaler\n",
    "\n",
    "# --- Prediction function ---\n",
    "### <<< CHANGE: Function now just takes the final data and scalers\n",
    "def predict_next_hour(historical_data_df, model, feature_scaler, target_scaler, feature_cols, window_size=48, device='cuda'):\n",
    "    model.eval()\n",
    "\n",
    "    # Get the last 'window_size' rows from the *entire* historical dataset\n",
    "    last_seq_df = historical_data_df[feature_cols].iloc[-window_size:]\n",
    "\n",
    "    # Scale the sequence using the *already-fit* scaler\n",
    "    last_seq_scaled = feature_scaler.transform(last_seq_df)\n",
    "\n",
    "    # Convert to tensor and add batch dimension\n",
    "    last_seq_tensor = torch.tensor(last_seq_scaled, dtype=torch.float32).unsqueeze(0).to(device) # [1, seq, features]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_scaled = model(last_seq_tensor) # [1, 1]\n",
    "\n",
    "    # Inverse scale the prediction\n",
    "    pred = target_scaler.inverse_transform(pred_scaled.cpu().numpy())[0][0]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import copy\n",
    "\n",
    "# Assume 'df' is your fully loaded HOURLY DataFrame\n",
    "\n",
    "### CHANGE ###\n",
    "# New function to resample hourly data to daily and create features.\n",
    "def create_daily_features(df):\n",
    "    \"\"\"\n",
    "    Resamples hourly data to daily, creates a future target, and engineers daily features.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['datetime_utc'] = pd.to_datetime(df['datetime_utc'])\n",
    "    df = df.set_index('datetime_utc').sort_index()\n",
    "\n",
    "    # 1. Define aggregation rules for resampling\n",
    "    agg_dict = {\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last',\n",
    "        'volume': 'sum',\n",
    "        'weighted_sentiment': 'mean' # Daily average sentiment\n",
    "    }\n",
    "    # For all other interest rate columns, we take the last known value of the day\n",
    "    for col in df.columns:\n",
    "        if col not in agg_dict:\n",
    "            agg_dict[col] = 'last'\n",
    "\n",
    "    # 2. Resample the data to daily frequency\n",
    "    daily_df = df.resample('D').agg(agg_dict)\n",
    "\n",
    "    # 3. Handle missing days (e.g., weekends) by forward-filling data\n",
    "    daily_df = daily_df.ffill()\n",
    "    # Drop any remaining NaNs at the beginning\n",
    "    daily_df = daily_df.dropna(how='all')\n",
    "\n",
    "    # 4. Create the target variable: the next day's close price\n",
    "    daily_df['target'] = daily_df['close'].shift(-1)\n",
    "\n",
    "    # 5. Remove the last row since it has no target\n",
    "    daily_df = daily_df.iloc[:-1]\n",
    "\n",
    "    # 6. Engineer features on the new DAILY data\n",
    "    daily_df['log_return'] = np.log(daily_df['close'] / daily_df['close'].shift(1))\n",
    "    daily_df['range'] = daily_df['high'] - daily_df['low']\n",
    "    daily_df['day_of_week'] = daily_df.index.dayofweek\n",
    "    daily_df['month'] = daily_df.index.month\n",
    "\n",
    "    # 7. Final cleanup\n",
    "    daily_df = daily_df.dropna().reset_index() # dropna for log_return and reset index\n",
    "\n",
    "    # 8. Define final feature and target columns\n",
    "    target_col = 'target'\n",
    "    # Features are all columns except the target and original merge_date\n",
    "    feature_cols = daily_df.columns.drop([target_col, 'merge_date', 'datetime_utc']).tolist()\n",
    "    \n",
    "    return daily_df, feature_cols, target_col\n",
    "\n",
    "### CHANGE ###\n",
    "# The Dataset class is slightly adjusted for clarity with the new target column.\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, target_col, window_size, feature_scaler, target_scaler):\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Scale features and target\n",
    "        feature_data_scaled = feature_scaler.transform(df[feature_cols])\n",
    "        target_data_scaled = target_scaler.transform(df[[target_col]])\n",
    "        \n",
    "        # Combine into one array for easier indexing\n",
    "        self.data = np.concatenate([feature_data_scaled, target_data_scaled], axis=1)\n",
    "        self.n_features = feature_data_scaled.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        # We need window_size history + 1 target\n",
    "        return len(self.data) - self.window_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Features are the first n_features columns\n",
    "        x = self.data[idx : idx + self.window_size, :self.n_features]\n",
    "        # Target is the last column, at the end of the window period\n",
    "        y = self.data[idx + self.window_size - 1, self.n_features:]\n",
    "        \n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# --- Transformer Model Class (Unchanged) ---\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, num_features, window_size, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_fc = nn.Linear(num_features, d_model)\n",
    "        self.pos_encoder = nn.Embedding(window_size, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_fc = nn.Linear(d_model, 1)\n",
    "        self.d_model = d_model\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_encoder(positions)\n",
    "        x = self.input_fc(x) + pos_emb\n",
    "        x = self.norm(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x[:, -1, :]\n",
    "        out = self.output_fc(x)\n",
    "        return out\n",
    "\n",
    "### CHANGE ###\n",
    "# train_transformer is updated to use the new Dataset constructor\n",
    "def train_transformer(train_df, val_df, feature_cols, target_col, window_size,\n",
    "                      epochs=50, batch_size=32, lr=1e-4, device='cuda', d_model=64, num_layers=2):\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "    feature_scaler.fit(train_df[feature_cols])\n",
    "    target_scaler.fit(train_df[[target_col]])\n",
    "    \n",
    "    train_dataset = TimeSeriesDataset(\n",
    "        df=train_df, feature_cols=feature_cols, target_col=target_col,\n",
    "        window_size=window_size, feature_scaler=feature_scaler, target_scaler=target_scaler)\n",
    "    val_dataset = TimeSeriesDataset(\n",
    "        df=val_df, feature_cols=feature_cols, target_col=target_col,\n",
    "        window_size=window_size, feature_scaler=feature_scaler, target_scaler=target_scaler)\n",
    "        \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = TimeSeriesTransformer(num_features=len(feature_cols), window_size=window_size, d_model=d_model,num_layers=num_layers).to(device)\n",
    "    # ... (rest of the training loop is unchanged) ...\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        # ... training steps ...\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                y_pred = model(x_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Val Loss: {val_loss:.6f}\")\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model, feature_scaler, target_scaler\n",
    "\n",
    "### CHANGE ###\n",
    "# Renamed function for clarity\n",
    "def predict_next_day(historical_data_df, model, feature_scaler, target_scaler, feature_cols, window_size, device='cuda'):\n",
    "    model.eval()\n",
    "    last_seq_df = historical_data_df[feature_cols].iloc[-window_size:]\n",
    "    last_seq_scaled = feature_scaler.transform(last_seq_df)\n",
    "    last_seq_tensor = torch.tensor(last_seq_scaled, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred_scaled = model(last_seq_tensor)\n",
    "    pred = target_scaler.inverse_transform(pred_scaled.cpu().numpy())[0][0]\n",
    "    return pred\n",
    "\n",
    "\n",
    "### CHANGE ###\n",
    "# --- MAIN SCRIPT EXECUTION FLOW (EXAMPLE FOR DAILY PREDICTION) ---\n",
    "if __name__ == '__main__':\n",
    "    # Assume 'df' is your initial HOURLY dataframe\n",
    "\n",
    "    # 1. Create daily aggregated data and features\n",
    "    daily_processed_df, feature_cols, target_col = create_daily_features(df)\n",
    "    \n",
    "    print(f\"Target column: {target_col}\")\n",
    "    print(f\"Number of daily features: {len(feature_cols)}\")\n",
    "\n",
    "    # 2. Split daily data into training and validation sets\n",
    "    train_size = int(len(daily_processed_df) * 0.8)\n",
    "    train_df = daily_processed_df.iloc[:train_size]\n",
    "    val_df = daily_processed_df.iloc[train_size:]\n",
    "    print(f\"Daily training data shape: {train_df.shape}\")\n",
    "    print(f\"Daily validation data shape: {val_df.shape}\")\n",
    "\n",
    "    # 3. Define hyperparameters. Window size now refers to DAYS.\n",
    "    # Using 30 days of history to predict the next day.\n",
    "    WINDOW_SIZE = 7\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # 4. Run training\n",
    "    best_model, feature_scaler, target_scaler = train_transformer(\n",
    "        train_df=train_df, val_df=val_df,\n",
    "        feature_cols=feature_cols, target_col=target_col,\n",
    "        window_size=WINDOW_SIZE, device=DEVICE\n",
    "    )\n",
    "    \n",
    "    # 5. Make a prediction for the next day\n",
    "    predicted_close = predict_next_day(\n",
    "        historical_data_df=daily_processed_df, # Use the full daily history\n",
    "        model=best_model,\n",
    "        feature_scaler=feature_scaler, target_scaler=target_scaler,\n",
    "        feature_cols=feature_cols,\n",
    "        window_size=WINDOW_SIZE, device=DEVICE\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Prediction ---\")\n",
    "    print(f\"Predicted Close Price for the NEXT DAY: {predicted_close:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239e8ce9",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e6179",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "WINDOW_SIZE = 6 # Using hours to predict the next 1\n",
    "TARGET_COL = 'close'\n",
    "\n",
    "# 1. Feature Engineering\n",
    "features_df, feature_cols, target_col = create_daily_features(df)\n",
    "\n",
    "# 2. Split Data (Time-based)\n",
    "# Use 80% for training, 20% for validation\n",
    "split_idx = int(len(features_df) * 0.8)\n",
    "\n",
    "train_df = features_df.iloc[:split_idx]\n",
    "val_df = features_df.iloc[split_idx:]\n",
    "\n",
    "print(f\"Total historical samples: {len(features_df)}\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "\n",
    "# 3. Train Model\n",
    "model, f_scaler, t_scaler = train_transformer(\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    device=DEVICE,\n",
    "    d_model=64,\n",
    "    num_layers=2\n",
    ")\n",
    "\n",
    "# 4. Predict the *actual* next hour\n",
    "# We use the *full* historical DF to get the most recent data\n",
    "predicted_price = predict_next_day(\n",
    "    historical_data_df=features_df, # Use all available history\n",
    "    model=model,\n",
    "    feature_scaler=f_scaler,\n",
    "    target_scaler=t_scaler,\n",
    "    feature_cols=feature_cols,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    device=DEVICE\n",
    ")\n",
    "true_close = df['close'].iloc[-1]\n",
    "print(\"\\n--- Final Prediction ---\")\n",
    "print(f\"Predicted next day price: {predicted_price:.2f}\")\n",
    "print(f\"    Actual next day price: {true_close:.2f}\")\n",
    "print(f\"                   Error: {predicted_price - true_close:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c124b70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
